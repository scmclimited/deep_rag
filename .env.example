# For local scripts:
# DB_HOST=localhost

# For Docker Compose runs (recommended):
DB_HOST=db

DB_PORT=5432
DB_USER=user_here
DB_PASS=password_here
DB_NAME=chosen_rag_db_name

# Common knobs
LLM_TEMPERATURE=0.2

# Choose your provider
LLM_PROVIDER=gemini        
# or: llava | openai | ollama

GEMINI_API_KEY=AIzaSyB**************
GEMINI_MODEL=gemini-2.5-flash


# - Use ollama to serve open-source/smaller task-specific models
# - For local ollama model inference, you must execute the following after downloading ollama 
# - Model used here is llama3:8b or choose your model preference

# Pull the chosen llama model
## ollama pull llama3:8b 

# wait for execution and then serve the model locally
# update the ENV variable to the localhost:port combo

## ollama serve

# Ollama (local)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b

LLAVA_URL=http://localhost:11434
LLAVA_MODEL=llava-hf/llava-1.5-7b-hf

# OpenAI (hosted)
OPENAI_API_KEY=sk-xxxxxxx
OPENAI_MODEL=gpt-4o-mini



