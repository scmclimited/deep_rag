{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c13e2aa",
   "metadata": {},
   "source": [
    "## Summarize & Extract text (PDF files + Images) using Spark and Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1eb580",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc082886",
   "metadata": {},
   "source": [
    "This notebook shows how to perform summarization using Gemini for a set of PDF files and Images\n",
    "\n",
    "#### **Steps**\n",
    "Using Spark, \n",
    "1) It reads the table of the [Contract Understanding Atticus Dataset (CUAD)](https://www.atticusprojectai.org/cuad) dataset located in the [gs://dataproc-metastore-public-binaries/cuad_v1/full_contract_pdf/](https://console.cloud.google.com/storage/browser/dataproc-metastore-public-binaries/cuad_v1)  \n",
    "   We will create a metadata table poiting to the paths of the image files in the bucket.  \n",
    "3) It calls [Vertex AI Gemini API](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart#try_text_prompts) to summarize the text.\n",
    "4) It saves the output to BigQuery\n",
    "\n",
    "#### Related content\n",
    "\n",
    "- [Design summarization prompts](https://cloud.google.com/vertex-ai/docs/generative-ai/text/summarization-prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3db59e",
   "metadata": {},
   "source": [
    "## ENV Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63680ded",
   "metadata": {},
   "source": [
    "#### Identity and Access Management (IAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ca9f4",
   "metadata": {},
   "source": [
    "Make sure the service account running this notebook has the required permissions:\n",
    "\n",
    "- **Run the notebook**\n",
    "  - AI Platform Notebooks Service Agent\n",
    "  - Notebooks Admin\n",
    "  - Vertex AI Administrator\n",
    "- **Read files from bucket**\n",
    "  - Storage Object Viewer\n",
    "- **Run Dataproc jobs**\n",
    "  - Dataproc Service Agent\n",
    "  - Dataproc Worker\n",
    "- **Call Google APIs (Gemini)**\n",
    "  - Service Usage Consumer\n",
    "  - VisionAI Admin\n",
    "- **BigQuery**\n",
    "  - BigQuery Data Editor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2fa06",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using Dataproc Serverless, installed packages are automatically available on all nodes\n",
    "!pip3 install --upgrade -q google-cloud-aiplatform google-genai \"protobuf~=4.25.3\" \"numpy~=1.26.4\" \n",
    "# When using a Dataproc cluster, you will need to install these packages during cluster creation: https://cloud.google.com/dataproc/docs/tutorials/python-configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7133c",
   "metadata": {},
   "source": [
    "### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get credentials to authenticate with Google APIs\n",
    "credentials, project_id = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc249a",
   "metadata": {},
   "source": [
    "## Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7701fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDF/Image files summarization using Gemini\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8b803",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343853c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the maximum number of files you want to consider\n",
    "limit_files = 5\n",
    "\n",
    "# BigQuery\n",
    "output_dataset_bq = \"output_dataset\" # create the BigQuery dataset beforehand\n",
    "output_table_bq = \"summaries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4dd3bb",
   "metadata": {},
   "source": [
    "## Read Online CUAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2a6f5",
   "metadata": {},
   "source": [
    "#### Read CUAD V1 dataset from metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARIES_BUCKET_PATH = \"gs://dataproc-metastore-public-binaries/cuad_v1/full_contract_pdf/\"\n",
    "cuad_v1_df = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").load(BINARIES_BUCKET_PATH).limit(limit_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70910d",
   "metadata": {},
   "source": [
    "|                path|    modificationTime| length|             content|\n",
    "|--------------------|--------------------|-------|--------------------|\n",
    "|gs://dataproc-met...|2023-05-15 20:53:...|3683550|[25 50 44 46 2D 3...|\n",
    "|gs://dataproc-met...|2023-05-15 20:53:...|2881262|[25 50 44 46 2D 3...|\n",
    "|gs://dataproc-met...|2023-05-15 20:54:...|1778356|[25 50 44 46 2D 3...|\n",
    "|gs://dataproc-met...|2023-05-15 20:53:...|1557129|[25 50 44 46 2D 3...|\n",
    "|gs://dataproc-met...|2023-05-15 20:53:...|1452180|[25 50 44 46 2D 3...|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4488071",
   "metadata": {},
   "source": [
    "### Summarize pages using Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_predict(gcs_pdf_uri, model_name=\"gemini-2.0-flash\", max_retries=3, initial_delay=1):\n",
    "    \n",
    "    import time\n",
    "    from google import genai\n",
    "    from google.genai import types\n",
    "    \n",
    "    client = genai.Client(\n",
    "        vertexai=True,\n",
    "        project=project_id,\n",
    "        location=\"us-central1\"\n",
    "    )\n",
    "    \n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        response_mime_type = \"text/plain\"\n",
    "    )\n",
    "\n",
    "    contents = [\n",
    "        types.Part.from_uri(\n",
    "            file_uri=gcs_pdf_uri,\n",
    "            mime_type='application/pdf',\n",
    "        ),\n",
    "        \"\"\" You an expert in reading contracts, articles, agreements, or text in general.\n",
    "            You are able to create concise summaries of the text provided to you.\n",
    "            Provide a summary about the attached pdf with about 3 sentences with the most important information from the text.\n",
    "            Summary:\n",
    "        \"\"\"\n",
    "  ]\n",
    "    \n",
    "    retries, delay = 0, initial_delay\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            response = client.models.generate_content(model=model_name,\n",
    "                                                      contents=contents,\n",
    "                                                      config=generate_content_config)\n",
    "            \n",
    "            return response.text\n",
    "        except Exception:\n",
    "            if retries == max_retries:\n",
    "                return\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "            retries += 1\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d71260",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_text = udf(gemini_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1071e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = cuad_v1_df.withColumn(\"summary\", summarize_text(cuad_v1_df[\"path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df.show(5,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f3aa8",
   "metadata": {},
   "source": [
    "|                                              path|       modificationTime| length|                                           content|                                           summary|\n",
    "|--------------------------------------------------|-----------------------|-------|--------------------------------------------------|--------------------------------------------------|\n",
    "|gs://dataproc-metastore-public-binaries/cuad_v1...|2023-05-15 20:53:55.891|3683550|[25 50 44 46 2D 31 2E 34 0A 25 E2 E3 CF D3 0A 3...|Here is a summary of the provided document:\\n\\n...|\n",
    "|gs://dataproc-metastore-public-binaries/cuad_v1...|2023-05-15 20:53:57.195|2881262|[25 50 44 46 2D 31 2E 35 0A 25 E2 E3 CF D3 0A 0...|This document is a promotion and distribution a...|\n",
    "|gs://dataproc-metastore-public-binaries/cuad_v1...|2023-05-15 20:54:00.609|1778356|[25 50 44 46 2D 31 2E 35 0A 25 E2 E3 CF D3 0A 0...|This document is a strategic alliance agreement...|\n",
    "|gs://dataproc-metastore-public-binaries/cuad_v1...|2023-05-15 20:53:57.902|1557129|[25 50 44 46 2D 31 2E 35 0A 25 E2 E3 CF D3 0A 0...|This PDF is a collaboration agreement between t...|\n",
    "|gs://dataproc-metastore-public-binaries/cuad_v1...|2023-05-15 20:53:57.659|1452180|[25 50 44 46 2D 31 2E 34 0D 25 C8 C8 C8 C8 C8 C...|This is a Transportation Services Agreement bet...|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b15a24",
   "metadata": {},
   "source": [
    "## Retrieve from Local Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5db0c",
   "metadata": {},
   "source": [
    "### Read from Local Vector Database\n",
    "\n",
    "You can use `spark.read.format(\"jdbc\")` to read chunks from your local PostgreSQL vector database.\n",
    "\n",
    "**Note:** The `vector(768)` type from pgvector and `JSONB` types may need special handling. Consider casting them to text/arrays in your SQL query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a892b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read chunks from local PostgreSQL vector database\n",
    "# Note: You'll need the PostgreSQL JDBC driver in your Spark environment\n",
    "# For Dataproc: Add --jars=gs://spark-lib/postgresql-42.7.1.jar to your Spark session\n",
    "# For local: Download postgresql-42.7.1.jar and add to Spark jars directory\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Database connection parameters (adjust based on your .env file)\n",
    "db_host = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "db_port = os.getenv(\"DB_PORT\", \"5432\")\n",
    "db_name = os.getenv(\"DB_NAME\", \"deep_rag\")\n",
    "db_user = os.getenv(\"DB_USER\", \"postgres\")\n",
    "db_pass = os.getenv(\"DB_PASS\", \"postgres\")\n",
    "\n",
    "# JDBC URL\n",
    "jdbc_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "# Read chunks table\n",
    "# Note: Cast vector(768) to text/array and JSONB to text for Spark compatibility\n",
    "chunks_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"\"\"\n",
    "        (SELECT \n",
    "            chunk_id,\n",
    "            doc_id,\n",
    "            page_start,\n",
    "            page_end,\n",
    "            section,\n",
    "            text,\n",
    "            is_ocr,\n",
    "            is_figure,\n",
    "            content_type,\n",
    "            image_path,\n",
    "            emb::text as emb_text,  -- Cast vector to text\n",
    "            meta::text as meta_text  -- Cast JSONB to text\n",
    "        FROM chunks\n",
    "        LIMIT 1000) AS chunks_subquery\n",
    "    \"\"\") \\\n",
    "    .option(\"user\", db_user) \\\n",
    "    .option(\"password\", db_pass) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "chunks_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12571fc6",
   "metadata": {},
   "source": [
    "### Alternative: Read with Custom Query (More Flexible)\n",
    "\n",
    "You can also use a custom SQL query to read specific chunks or join with documents table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Read chunks with documents metadata using custom query\n",
    "chunks_with_docs_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", \"\"\"\n",
    "        SELECT \n",
    "            c.chunk_id,\n",
    "            c.doc_id,\n",
    "            d.title as doc_title,\n",
    "            d.source_path,\n",
    "            c.page_start,\n",
    "            c.page_end,\n",
    "            c.text,\n",
    "            c.content_type,\n",
    "            c.is_ocr,\n",
    "            c.is_figure,\n",
    "            c.emb::text as embedding_text,  -- Vector as text\n",
    "            c.meta::text as chunk_meta       -- JSONB as text\n",
    "        FROM chunks c\n",
    "        LEFT JOIN documents d ON c.doc_id = d.doc_id\n",
    "        WHERE c.content_type = 'text'  -- Filter by content type\n",
    "        LIMIT 1000\n",
    "    \"\"\") \\\n",
    "    .option(\"user\", db_user) \\\n",
    "    .option(\"password\", db_pass) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "chunks_with_docs_df.show(5, truncate=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fb619",
   "metadata": {},
   "source": [
    "### Important Considerations:\n",
    "\n",
    "1. **PostgreSQL JDBC Driver**: You need the PostgreSQL JDBC driver (postgresql-42.7.1.jar) in your Spark classpath\n",
    "   - For Dataproc: Use `--jars=gs://spark-lib/postgresql-42.7.1.jar` when creating Spark session\n",
    "   - For local: Download and add to Spark jars directory\n",
    "\n",
    "2. **Vector Type Handling**: The `vector(768)` type from pgvector needs to be cast to text or array\n",
    "   - Use `emb::text` to get vector as text representation\n",
    "   - Or use `emb::float8[]` to get as array (if supported)\n",
    "\n",
    "3. **JSONB Handling**: Cast JSONB to text: `meta::text`\n",
    "\n",
    "4. **Performance**: For large datasets, consider:\n",
    "   - Using `partitionColumn`, `lowerBound`, `upperBound` for parallel reads\n",
    "   - Adding `numPartitions` option for better parallelism\n",
    "   - Filtering in SQL query rather than after loading\n",
    "\n",
    "5. **Connection Pooling**: For high-concurrency, consider using connection pooling options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc82382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Parallel read with partitioning (for large datasets)\n",
    "# This splits the read across multiple Spark partitions for better performance\n",
    "# Note: Partition by chunk_id (UUID) for parallel reads\n",
    "\n",
    "chunks_parallel_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"\"\"\n",
    "        (SELECT \n",
    "            chunk_id,\n",
    "            doc_id,\n",
    "            text,\n",
    "            emb::text as emb_text,\n",
    "            meta::text as meta_text\n",
    "        FROM chunks) AS chunks_subquery\n",
    "    \"\"\") \\\n",
    "    .option(\"user\", db_user) \\\n",
    "    .option(\"password\", db_pass) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"partitionColumn\", \"chunk_id\") \\\n",
    "    .option(\"lowerBound\", \"0\") \\\n",
    "    .option(\"upperBound\", \"1000000\") \\\n",
    "    .option(\"numPartitions\", \"10\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Number of partitions: {chunks_parallel_df.rdd.getNumPartitions()}\")\n",
    "print(f\"Total chunks: {chunks_parallel_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1a374",
   "metadata": {},
   "source": [
    "## Save to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ebc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summaries_df.write \\\n",
    "#             .format(\"bigquery\") \\\n",
    "#             .option(\"table\", f\"{project_id}.{output_dataset_bq}.{output_table_bq}\") \\\n",
    "#             .option(\"writeMethod\", \"direct\") \\\n",
    "#             .mode(\"overwrite\") \\\n",
    "#             .save()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
