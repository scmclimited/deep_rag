services:
  db:
    image: pgvector/pgvector:pg16
    container_name: deep_rag_pgvector
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASS}
      POSTGRES_DB: ${DB_NAME}
      # Increase max connections for concurrent queries
      POSTGRES_MAX_CONNECTIONS: 200
      # Performance tuning for high-concurrency workloads
      POSTGRES_SHARED_BUFFERS: 4GB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 10GB
      POSTGRES_MAINTENANCE_WORK_MEM: 1GB
      POSTGRES_WORK_MEM: 64MB
      POSTGRES_MAX_WORKER_PROCESSES: 16
      POSTGRES_MAX_PARALLEL_WORKERS_PER_GATHER: 4
      POSTGRES_MAX_PARALLEL_WORKERS: 16
    ports:
      - "${DB_PORT:-5432}:5432"
    # Increase resource limits for high-performance workloads
    deploy:
      resources:
        limits:
          cpus: '16.0'
          memory: 16G
        reservations:
          cpus: '4.0'
          memory: 4G
    volumes:
      # Named volume for persistent data (remove to start fresh)
      - deep_rag_db_data:/var/lib/postgresql/data
      # Schema with all features integrated
      - ./vector_db/schema_multimodal.sql:/docker-entrypoint-initdb.d/01_schema_multimodal.sql
      # Custom PostgreSQL configuration for high-performance workloads
      - ./vector_db/postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      - deep_rag_network
  api:
    build:
      context: ./deep_rag_backend
      dockerfile: Dockerfile
    container_name: deep_rag_api
    env_file:
      - .env
    environment:
      # Database connection - use service name 'db' when in Docker network
      DB_HOST: db
      DB_PORT: ${DB_PORT:-5432}
      DB_USER: ${DB_USER}
      DB_PASS: ${DB_PASS}
      DB_NAME: ${DB_NAME}
    depends_on:
      db:
        condition: service_healthy
    # GPU enablement (Compose v2)
    gpus: "all"
    # If your setup only honors Swarm-style device reservations, also add:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    volumes:
      # Map logs directory from container to local filesystem (production logs only)
      # This ensures logs written to /app/inference/graph/logs/ inside the container
      # are immediately accessible at ./deep_rag_backend/inference/graph/logs/ on your local machine
      # Test logs (inference/graph/logs/test/) are excluded - they are not mounted and not copied to image
      - ./deep_rag_backend/inference/graph/logs:/app/inference/graph/logs
      # Graph visualization artifacts are created in container's /app/inference/graph/artifacts (not mounted)
      # Use docker cp to extract if needed: docker cp deep_rag_api:/app/inference/graph/artifacts ./deep_rag_backend/inference/graph/artifacts
    working_dir: /app
    # Use entrypoint.sh which handles CUDA verification and starts the service
    # entrypoint.sh uses: uvicorn inference.service:app
    healthcheck:
      test: ["CMD-SHELL", "python - <<'PY'\nimport socket; s=socket.socket(); s.settimeout(2); s.connect(('127.0.0.1',8000)); print('ok')\nPY"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 30s
    networks:
      - deep_rag_network

  frontend:
    build:
      context: ./deep_rag_frontend_vue
      dockerfile: Dockerfile
    container_name: deep_rag_frontend
    depends_on:
      db:
        condition: service_healthy
      api:
        condition: service_healthy
    env_file:
      - .env
    environment:
      # API connection - connect to backend API in same network
      VITE_API_BASE_URL: http://api:8000
    ports:
      - "${FRONTEND_PORT:-5173}:80"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - deep_rag_network

volumes:
  deep_rag_db_data:

networks:
  deep_rag_network:
    driver: bridge